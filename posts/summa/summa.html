<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Morten Lysgaard">
  <meta name="author" content="Morten Lysgaard">
  <title>Application of using MPI for distributed numerical computation; solving Poisson's equation.</title>
  <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              showMathMenu: false,
              jax: ["input/TeX","output/CommonHTML"],
              extensions: ["tex2jax.js"],
              messageStyle: "none",
                tex2jax: {
                  processEscapes: true,
                  ignoreClass: "tex2jax_ignore",
                  processClass: "math"
                },
              TeX: { 
                extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js","autobold.js"],
                Macros: { 
                    R: '{\\mathbb{R}}', 
                    N: '{\\mathbb{N}}', 
                    Z: '{\\mathbb{Z}}', 
                    C: '{\\mathbb{C}}', 
                    F: '{\\mathbb{F}}', 
                    argmin: '{\\mathop{\\operatorname*{arg\\,min}}}',
                    argmax: '{\\mathop{\\operatorname*{arg\\,max}}}',
                    mex: '{\\mathop{\\operatorname{mex}}}', 
                    lcm: '{\\mathop{\\operatorname{lcm}}}',
                    bigtriangleright: '{\\mathop{\\Large \\triangleright}}',
                    bigtriangleleft: '{\\mathop{\\Large \\triangleleft}}',
                    set: ['\\left\\{ #1 \\right\\}',1],
                    floor: ['\\left\\lfloor #1 \\right\\rfloor',1],
                    ceil:  ['\\left\\lceil #1 \\right\\rceil',1],
                    abs:  ['\\left| #1 \\right|',1]
                 } 
                },
                "HTML-CSS": { fonts: ["STIX"] }
            });
        </script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script>
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
  <link rel="stylesheet" href="/css/default.css" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-24607697-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-24607697-1');
  </script>

</head>

<body class="tex2jax_ignore">
  <header class="hide-on-print">
    <div id="blog-title">
      <a href="/">Morten Lysgaard</a>
    </div>
    <nav>
      <ul style="display: inline-block;">
        <li><a href="/">Home</a></li>
        <!--li><a href="/pages/cv.html">CV</a></li-->
        <li><a href="/pages/trajectory-optimization-toolbox.html">Trajectory Optimization Toolbox</a></li>
        <li><a href="/pages/robotics-simulator.html">Robotics Simulator</a></li>
        <li><a href="/posts.html">Posts</a></li>
        <!--li><a href="/archive.html">Archive</a></li-->
      </ul>
    </nav>
  </header>
  <article>
    <h1 id="article-title">Application of using MPI for distributed numerical computation; solving Poisson's equation.</h1>
    <br/>
<div>
<p>This is a web-based version of a report I made for a parallel computation course I took while on exchange at <a href="http://www.qut.edu.au/">Queensland University of Technology, Australia</a>. Together with the <a href="https://github.com/molysgaard/poisson">open-sourced code</a> referenced in the <a href="https://github.com/molysgaard/poisson/raw/master/README.pdf">report</a> it is a good example of what is needed for distributed memory, high performance computing, algorithm development. Theory, design, implementation, test design. Verification of correctness against known correct reference implementations. Verification of performance trough collection of nondeterministic test data and analysis of these data using statistics.</p>
<p>The solver is written in plain C taking use of technologies as OpenMP, BLAS and OpenMPI. Compared to a naively written single core solver written in C my solution had a speedup of 200 on a 50 core cluster. Compared to a bleedingly optimized sequential implementation taking use of the Intel MKL library for hand tuned matrix multiply code a speedup of 20 was measured. The distributed nature of the solver can also cope with much bigger problems than the sequential implementation. One run was done over 64 nodes each using 8 cores, that is 512 cores. The problem was a finite difference discretization with 32000x32000 grid points. Calculating the solution took 2 minutes. A sequential implementation would possibly use days to finish.</p>
<h1 id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>The first use of computers was for doing simple calculations at a higher rate than a human possibly could. Today computers are no longer just calculators but present everywhere around us, integrating to our lives to a bigger and bigger extent. We are past the era of looking on computers as big calculators, but in spite of that scientific computing and the use of computers for doing ever bigger calculations is just increasing. Today it is often much cheaper to do numerical experiments, simulating the nature, instead of setting up a real experiment. Numerical experiments are usually cheaper, less dangerous and easier to analyse the results from. Fields that use this extensively are military, for nuclear explosion simulations and meteorologists for simulation the weather.</p>
<p>Almost all computer aided design is today tested numerically before any physical model is made. Planes are tested for their flight characteristics, cars for their aerodynamic properties, engines for their thermodynamics etc. All of these applications are very computational intensive and on the really big scale, like weather forecasts, they require immense computational power, clever algorithms and programming techniques. This report covers a sample problem in scientific computing. Solving the Poisson equation for a big problem size. This requires an approach very different from a naive sequential implementation to get good performance.</p>
<h1 id="sec:sol_steps"><span class="header-section-number">2</span> Problem description</h1>
<p>Poisson’s equation is given as <span class="math display">\[-\Delta u = f\]</span> where <span class="math inline">\(\Delta = \nabla^2\)</span> is the Laplace operator, u and f are real or complex valued functions in a euclidean space. The equation is often written as <span class="math display">\[-\nabla^2 u = f.\]</span> In two dimensions the equation can be written as <span class="math display">\[-\left( {\frac{\partial^2 }{\partial x^2}} + {\frac{\partial^2 }{\partial y^2}} \right) u(x,y) = f(x,y).\]</span></p>
<p>In sec. <a href="#sec:math_der">14</a> we derive the chosen solution method of this equation. For the parallel analysis it suffices to know that one ends up with a three step solution process.</p>
<h2 id="step-1"><span class="header-section-number">2.1</span> step 1</h2>
<p>Form the two matrix matrix products <span class="math display">\[\tilde{G} = Q^\top G Q.\]</span></p>
<h2 id="step-2"><span class="header-section-number">2.2</span> step 2</h2>
<p>solve <span class="math display">\[\Lambda \tilde{U} + \tilde{U} \Lambda = \tilde{G}\]</span> or <span class="math display">\[\tilde{u}_{i,j} = \frac{\tilde{g}_{i,j}}{\lambda_i + \lambda_j}.\]</span></p>
<h2 id="step-3"><span class="header-section-number">2.3</span> step 3</h2>
<p>Compute the matrix matrix product <span class="math display">\[U = Q \tilde{U} Q^\top.\]</span></p>
<p>Where <span class="math inline">\(G\)</span> and <span class="math inline">\(U\)</span> is a discrete version of <span class="math inline">\(h^2 f\)</span> and <span class="math inline">\(u\)</span> respectively, <span class="math inline">\(Q\)</span> is the eigen vectors of the Laplace operator, <span class="math inline">\(\lambda\)</span> is a diagonal matrix of the eigen values of the discretized Laplace operator and <span class="math inline">\(\tilde{A} = Q^\top A Q\)</span>.</p>
<h1 id="computational-complexity"><span class="header-section-number">3</span> Computational complexity</h1>
<p>The asymptotic complexity of this method is bounded by the most expensive step in the algorithm. Given a 2D grid of size <span class="math inline">\(n-1 \times n-1\)</span> we can denote the following complexity to each step.</p>
<h2 id="step-1-1"><span class="header-section-number">3.1</span> step 1</h2>
<p>2 matrix matrix products each have complexity <span class="math inline">\(O(n^3)\)</span>.</p>
<h2 id="step-2-1"><span class="header-section-number">3.2</span> step 2</h2>
<p><span class="math inline">\(n^2\)</span> constant operations, <span class="math inline">\(O(n^2)\)</span>.</p>
<h2 id="step-3-1"><span class="header-section-number">3.3</span> step 3</h2>
<p>2 matrix matrix products each have complexity <span class="math inline">\(O(n^3)\)</span>.</p>
<p>This means that this solution method has <span class="math inline">\(O(n^3)\)</span> complexity and a sequential implementation will have asymptotic running time of <span class="math inline">\(n^3\)</span> of the problem size <span class="math inline">\(n\)</span>. Here the problem size <span class="math display">\[n \leq \max(n_x,n_y)\]</span> where <span class="math inline">\(n_x\)</span> and <span class="math inline">\(n_y\)</span> is the number of grid points in our finite difference approximation in the x and y direction respectively.</p>
<h1 id="survey-of-an-alternate-solution-method"><span class="header-section-number">4</span> Survey of an alternate solution method</h1>
<p>The simplest method of solving the Poisson equation is to apply a 5 point stencil on your domain iteratively. This means that each cell in the grid gives a fraction of its own value to its neighbors. Then this is done for each cell iteratively until the grid does not change more than a given threshold.</p>
<p>This method has the same asymptotic operation cost, <span class="math inline">\(O(n^3)\)</span> and better memory requirement <span class="math inline">\(O(n^2)\)</span>. However it does not lend itself well to parallelization. To parallelize this one would split up the domain in blocks that each node can take care of. The five point stencil would then be run in parallel on each of the nodes. At the end of each iteration phase a global syncronization would have to occur for all the nodes in the compute grid to know the boundary values to its own domain. This would lead to <span class="math inline">\(O(n)\)</span> all to all communication steps where each node has to send and receive <span class="math display">\[4\frac{n}{\sqrt{p}}\]</span> elements, where <span class="math inline">\(p\)</span> is the number of compute nodes the domain is distributed on. This synchronization would make the algorithm way to slow for any practical size of the problem compared to the direct diagonalization method.</p>
<h1 id="analysis-of-parallelizable-sections."><span class="header-section-number">5</span> Analysis of parallelizable sections.</h1>
<p>The presented solution method sec. <a href="#sec:sol_steps">2</a> is not a obvious candidate for parallelization since a large majority of the time is used doing matrix multiplications. This means that to make this algorithm scale well a large scale parallelized matrix multiply has to implemented. A naive parallelization on a single SMP machine was rejected as it would not let the algoritm scale to the huge scales that are realistic in applications such as weather forecasts. After some research it was found that that there exists a algorithm for this, <span class="citation" data-cites="summa">[<a href="#ref-summa">1</a>]</span>, discovered in 1997.</p>
<h2 id="the-summa-algorithm"><span class="header-section-number">5.1</span> The SUMMA algorithm</h2>
<p>The SUMMA algorithm first appeared in the paper<span class="citation" data-cites="summa">[<a href="#ref-summa">1</a>]</span> with the same name in 1997. It was discovered by Robert A. van de Geijn and Jerrell Watts during research founded by NASA and Intel. The algorithm very quickly spread because of its relative simplicity compared to current parallel matrix multiply algorithms. An explanation of the algorithm follows.</p>
<p>In the SUMMA algorithm you form the matrix product <span class="math display">\[C = AB.\]</span> Where A is <span class="math inline">\(n \times k\)</span>, B <span class="math inline">\(k \times m\)</span> and C <span class="math inline">\(n \times m\)</span> matrices. Letting <span class="math inline">\(a_{i,j}\)</span> be the element on row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> in matrix <span class="math inline">\(A\)</span> we can write the computation as <span class="math display">\[c_{i,j} = \sum_k a_{i,k} b_{k,j}.\]</span> This is called the inner product formulation of matrix multiply since each <span class="math inline">\(c_{i,j}\)</span> is an inner product of a row in <span class="math inline">\(A\)</span> and a column in <span class="math inline">\(B\)</span>.</p>
<p>Although this is the normal formulation of matrix multiplication there exists another way to form it named the outer product formulation. Letting <span class="math inline">\(\bar{a_i}\)</span> be column <span class="math inline">\(i\)</span> in the matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(\bar{b_i}\)</span> be row <span class="math inline">\(i\)</span> in <span class="math inline">\(B\)</span> we can write the outer product formulation as follows <span class="math display">\[AB = \begin{pmatrix}
    \bar{a_1} &amp; \bar{a_2} &amp; \dots &amp; \bar{a_n}\\
     \end{pmatrix}
     \begin{pmatrix}
    \bar{b_1}\\
    \bar{b_2}\\
    \vdots\\
    \bar{b_n}\\
     \end{pmatrix}
     =\sum_{i=1}^n a_i \otimes b_i.\]</span> A nice property is that the formulation applies even if the elements of the matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are again matrices. We say that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are blocked matrices.</p>
<p>The compute nodes are arranged in a grid <span class="math display">\[
\begin{array}{|c|c|}
\hline
p_{1,1} &amp; p_{1,2}\\
\hline
p_{2,1} &amp; p_{2,2}\\
\hline
\end{array}
\]</span> in such a way that for a given matrix <span class="math display">\[
A=\begin{bmatrix}
  a_{1,1} &amp; a_{1,2} &amp; a_{1,3} &amp; a_{1,4} \\
  a_{2,1} &amp; a_{2,2} &amp; a_{2,3} &amp; a_{2,4} \\
  a_{3,1} &amp; a_{3,2} &amp; a_{3,3} &amp; a_{3,4} \\
  a_{4,1} &amp; a_{4,2} &amp; a_{4,3} &amp; a_{4,4} \\
\end{bmatrix}
\]</span> the compute node <span class="math inline">\(p_{1,1}\)</span> has <span class="math display">\[
\begin{bmatrix}
  a_{1,1} &amp; a_{1,2} \\
  a_{2,1} &amp; a_{2,2} \\
\end{bmatrix}
\]</span> in memory. We call a compute nodes part of the matrix a block, and say the matrix <span class="math inline">\(A\)</span> is composed of several blocks. The same logic follows for the other nodes and matrices in the algorithm.</p>
<p>Pseudo code for each node is now <span class="math display">\[
\begin{aligned}
  &amp;C_{i,j} = 0\\
  &amp;\text{for } l=0 \text{ to } k-1\\
  &amp;\text{\enskip\enskip broadcast my part of } \bar{a_l} \text{ within my row}\\
  &amp;\text{\enskip\enskip broadcast my part of } \bar{b_l} \text{ within my column}\\
  &amp;\text{\enskip\enskip }C_{i,j} = C_{i,j} + \bar{a_l} \otimes \bar{b_l}\\
\end{aligned}
\]</span> This leaves us with <span class="math inline">\(2k\)</span> broadcast operations for the whole algorithm. This is to inefficient for the algorithm to be practical but there exists a pipe lining approach that can be used instead of broadcasting. In this version, each round of the algorithm the current column in A and row in B is sent to next node in the row and column respectively. The nodes on the end send to the first nodes. One can say that the nodes in each row and columns are organized in a logical ring. This ring is used to pass on information each step. Using this approach the communication is greatly reduced.</p>
<p>Another benefit is that on a good MPI implementation with dedicated hardware this lets the algorithm send data forward in the ring asynchronous while it computes a local outer product. This lessens the waiting for synchronization.</p>
<p>The SUMMA implementation in this report is a customized version of the one in the original paper<span class="citation" data-cites="summa">[<a href="#ref-summa">1</a>]</span>. It includes the blocking and pipe lining optimizations mentioned above.</p>
<h1 id="implementations"><span class="header-section-number">6</span> Implementations</h1>
<p>Two different implementations of the algorithm was done. This was to compare the running time of the parallel version with a reference sequential implementation.</p>
<h2 id="sequential-implementation"><span class="header-section-number">6.1</span> Sequential implementation</h2>
<p>The sequential version was implemented using vanilla C and the sequential Intel MKL libraray. The first version was without the Intel MKL library. But it was one order of magnitude slower. Something that proves how well written the intel MKL is. The sequential and parallel code share the same initialization and logic. The only difference is the calls to BLAS and the SUMMA algorithm. The sequential implementation landed on roughly 150 lines of C code.</p>
<h2 id="parallel-implementation"><span class="header-section-number">6.2</span> Parallel implementation</h2>
<p>In the parallel implementation “everything is allowed”. That is any library that could give increased performance would be used. The following tactic was developed.</p>
<p>Use the SUMMA algorithm to distribute the work across different nodes. Use Intel MKLs CBLAS implementation to do the internal matrix multiplications in the SUMMA algorithm.</p>
<p>There are three main reasons to spread the work across different nodes.</p>
<h2 id="memory-requirements"><span class="header-section-number">6.3</span> Memory requirements</h2>
<p>As the problem size, <span class="math inline">\(n\)</span>, increases the memory requirement increases at <span class="math inline">\(O(n^2)\)</span>. This means that for big problem sizes a single nodes memory is simply not enough to hold the whole problem.</p>
<h2 id="cache"><span class="header-section-number">6.4</span> Cache</h2>
<p>Distributing the work over several nodes gives more cache memory available. Even though a problem of a given size may not fill the available memory on a node it will very quickly fill the available cache. Since the cache memory gives several orders of magnitude faster access than normal memory the algorithm speeds up considerably from this.</p>
<h2 id="processors"><span class="header-section-number">6.5</span> Processors</h2>
<p>Even the biggest nodes in the QUT HPC machine only have 32 cores. Using only SMP techniques the maximum number of processors available is thus 32. This is enough for most applications but for this specific one where massive scalability is the key it is not enough.</p>
<p>The reason for using Intels MKL library for the internal matrix multiplication are many fold. First off it is the industry leading library for math kernels. After talks with the HPC staff at QUT it was discovered that MKL would normally outperform CUDAS BLAS, CUBLAS, because of the highly specialized code in MKL written for exactly the processor architectures that QUTs super computer uses. Intels MKL coupled with Intels C compiler, ICC, also supports linking options ta make it run in thread parallel mode. This made it possible to accurately control the granularity of both the distributed memory, MPI, and the shared memory, Intel MKL, parts to gain the highest possible performance.</p>
<p>The parallel version ended on roughly 370 lines of code. Compared to the sequential version this difference is almost solely because of the SUMMA algorithm and the scattering and gathering code.</p>
<h1 id="targeted-hardware"><span class="header-section-number">7</span> Targeted Hardware</h1>
<p>Since the application is meant to run on big problem sizes the natural target hardware was QUTs HPC cluster Lyra. Is is a SGI Altix XE cluster consisting of 128 compute nodes containing 1924 Intel Xeon Cores. Lyra is a heterogeneous cluster consisting of different nodes with different Xeon chips. The most power full cores where chosen for the task, the E5-2670@2.66(GHz) 64bit Intel Xeon processor 8 core processor. Lyra has 1472 of these cores.</p>
<h2 id="hardware-specific-tweaks"><span class="header-section-number">7.1</span> Hardware specific tweaks</h2>
<p>To get the most out of your program it is essential to customize it for your target architecture. For Lyra the following customization was done to maximize performance.</p>
<p>Instead of using the open source implementation of MPI, open-MPI, a proprietary SGI implementation, MPT, was used. The reason for this is that MPT is specially optimized for SGI clusters and uses the available hardware and proprietary network interconnects in a more optimal way than the generic open-MPI implementation does.</p>
<p>Another optimization that was used was usage of the Intel Math Kernel Library<span class="citation" data-cites="mkl">[<a href="#ref-mkl">2</a>]</span> for BLAS<span class="citation" data-cites="blas">[<a href="#ref-blas">3</a>]</span> routines instead of the open source BLAS implementations commonly available on UNIXes. This again hinges on that the MKL is specifically optimized for Intel processors and uses all the advanced vectorization, pipe lining and instruction level parallelism possible. To quote one of the MPC staff “Intel MKL usually sees better speed in linear algebra benchmarks than CUDAs BLAS implementation on our cluster.”</p>
<h1 id="libraries-software-and-tools-used"><span class="header-section-number">8</span> Libraries, software and tools used</h1>
<p>All development took place on Ubuntu Linux machine since the developer have 10 years experience with Linux and open source. This made tasks like installing development libraries etc. a breeze because of Linuxes package management systems.</p>
<p>The first iterations of code was targeted and written for a standard desktop machine. The Gnu C compiler and Open-MPI and the Gnu Scientific Library was installed and code developed. GCC together with Open-MPI worked well for debugging purposes when just running on one machine. For timing the real time library of Linux was used in the sequential implementation while MPIWallTime was used for the parallel one.</p>
<p>After a working MPI implementation was done the first small tests on the QUT HPC cluster was run. Here instead of GCC, Intels C compiler, ICC, was used. After talking to the HPC staff a switch to SGIs MPI implementation was also done. Lastly Intels MKL replaced the Gnu Scientific Library.</p>
<p>The parallel and sequential implementation where compiled with the same optimization flag, -O2.</p>
<h1 id="problems-encountered-during-implementation-and-solutions"><span class="header-section-number">9</span> Problems encountered during implementation and solutions</h1>
<p>When this project started the initial plan was to see if Haskell<span class="citation" data-cites="haskell">[<a href="#ref-haskell">4</a>]</span> would be robust and feature full enough to implement an industry grade numerical algorithm. For this the accelerate<span class="citation" data-cites="accelerate">[<a href="#ref-accelerate">5</a>]</span> library of Haskell was used and a functioning matrix multiply algorithm written to run on a CUDA back end. It was benchmarked but due to the nested parallelism needed to express matrix multiply in the domain specific language that accelerate exposes the performance was not even on par with a sequential implementation. The authors of accelerate, researchers at University of New South Wales, was contacted to get further insight in the problem, but they did not have an immediate solution.</p>
<p>After considering extending the accelerate library Haskell was scrapped as language of choice and focus given to a C implementation. Progress was made and over a few weeks a working SUMMA algorithm was implemented. A lot of time was used reasoning about the subtle interactions and transformations to indexes of the matrices when you create a distributed matrix. Several errors where found and corrected before a working implementation was achieved. One of the problem encountered was that the reference SUMMA algorithm from <span class="citation" data-cites="summa">[<a href="#ref-summa">1</a>]</span> uses BLAS subroutines written in FORTRAN. Since FORTRAN has column major ordering of its arrays all computations where transposed in the internal multiply routines of the SUMMA call. This was an especially hard bug to find because without this knowledge it is very hard to reason why the results are incorrect, when the algorithm step by step seemingly does the correct thing. To fix the problem it was chosen to use the CBLAS interface to BLAS. CBLAS wraps all BLAS calls to C style semantics and expects row major matrices.</p>
<p>The next part that proved hard was scattering and gathering matrices from the master node. Without this the different parts of the result matrix would be distributed across the cluster which is not sufficient if you want the SUMMA algorithm to be a plug in replacement for a BLAS dgemm call. Again the index transformations that occur when splitting up and gathering the matrix was the hard part and small errors where hidden in the indexes.</p>
<h2 id="correctness-of-algorithms"><span class="header-section-number">9.1</span> Correctness of algorithms</h2>
<p>For debugging the correctness of the SUMMA algorithm Matlabs implementation of matrix multiply was used. A random <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> matrix would be generated by the SUMMA program and written to disk. Then the SUMMA would be run and the answer matrix, <span class="math inline">\(C = A B\)</span>, would be written to disk. Afterwards a script would be run in Matlab to read <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> and cross reference it with the <span class="math inline">\(C\)</span> matrix. This was very help full to get the correctly implemented SUMMA algorithm.</p>
<p>After the SUMMA algorithm worked correctly an implementation of the Poisson solver was written using the SUMMA algorithm. Again a reference implementation was written in Matlab and both implementations where set to solve the same problem. Afterwards both solutions where plotted and compared to each other qualitatively and numerically to check for correctness. Several errors in the indexes of eigenvectors, eigenvalues and initialization where found in the SUMMA implementation and corrected. In the end the distributed MPI-SUMMA algorithm gave the same answer as the reference Matlab implementation down to machine precision and the distributed Poisson solver was considered correct.</p>
<h1 id="wall-time-analysis"><span class="header-section-number">10</span> Wall time analysis</h1>
<h2 id="what-was-measured"><span class="header-section-number">10.1</span> What was measured</h2>
<p>To be able to compare the sequential and parallel implementations some assumptions had to be done. The general rule of thumb was that whenever in doubt, give the sequential implementation the benefit.</p>
<p>The first part that had to be considered was if initialization of the problem should be included in the wall time measurements. Since the distributed implementation is able to initialize in a parallel fashion, all the nodes only do initialization for their small part of the problem, it was not included in the wall time measurements. This would have given the parallel implementation a seemingly unfair advantage.</p>
<p>The other part was if gathering the distributed results from the SUMMA algorithm should be counted in the wall time. It was decided that this should be included, the reasoning was that if the solution to the Poisson equation is to be used in further calculations on a single machine the whole distributed answer has to be gathered to the master node before the algorithm could continue.</p>
<h2 id="error-of-wall-time-measurements"><span class="header-section-number">10.2</span> Error of wall time measurements</h2>
<p>Because the application was run on a distributed machine also running lots of other processes special care had to be taken to get reproducible timing results.</p>
<p>Firstly the program was scheduled on the same type of machine every time. This way the heterogeneity of Lyra was ruled out as only the E5-2670 cores where used for time measurements. The second issue is that since the application is running MPI it is susceptible to network contention from other processes running on the cluster. Good data was still gathered using the following tactic. For each configuration of the problem, <span class="math inline">\(n\)</span>, nodes, mpi processes, cores etc. 10 instances of the same configuration is run. Afterwards a statistical analysis of the results where done in the software package R<span class="citation" data-cites="r-project">[<a href="#ref-r-project">6</a>]</span>, this way the noise and uncertainties of the wall time measurements could be countered and a reasonable level of precision achieved.</p>
<p>Since statistics where done on the wall run time some of the the plots showing times are box plots<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. The black bar in the box show the median, the box covers the two middle quadtiles. The whiskers show the maximum and minimum measured value and any circles shows outliers. Outliers are measurements that are so far from the rest that they probably are the product of an error, in this case another process from another HPC job interfering with our program.</p>
<h2 id="ratio-between-smp-and-distributed-memory-parallelism"><span class="header-section-number">10.3</span> Ratio between SMP and distributed memory parallelism</h2>
<p>This application consists of both distributed memory on the high level of the algorithm and SMP in the internal dgemm routines used by each MPI process. Because of this it was checked what ratio of <span class="math display">\[\frac{\text{MPI processes}}{\text{total cores}}\]</span> that performed optimally. Everything from 6 cores SMP per MPI process to 2 cores SMP per MPI process was tried and in the end only 2 cores OpenMP parallelism per MPI process performed best. Between double and triple the speed compared to 6 cores per MPI process. This does not seem intuitive because SMP parallelism usually is faster compared to distributed memory. Things like checking Intels MKL documentation<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> and manually setting the <code>OMP_NUM_THREADS</code> setting in the PBS job description did not change the running time. Theories go from Intels MKL not parallelizing fully on all available cores to maybe spawning too many threads. Future studies should be dedicated to this maybe unlocking even higher performance.</p>
<h2 id="sequential-implementation-wall-time"><span class="header-section-number">10.4</span> Sequential implementation wall time</h2>
<figure>
<img src="analysis/serial_walltime_mkl.svg" alt="Figure 1: Sequential implementation wall time as a function of problem size run on a single core." id="fig:serial_walltime" /><figcaption>Figure 1: Sequential implementation wall time as a function of problem size run on a single core.</figcaption>
</figure>
<h2 id="parallel-implementation-wall-time"><span class="header-section-number">10.5</span> Parallel implementation wall time</h2>
<figure>
<img src="analysis/parallel_problem_size_mkl.svg" alt="Figure 2: Parallel implementation wall time as a function of problem size. Run on 5 nodes each with 5 MPI processes and 10 cores." id="fig:parallel_walltime1" /><figcaption>Figure 2: Parallel implementation wall time as a function of problem size. Run on 5 nodes each with 5 MPI processes and 10 cores.</figcaption>
</figure>
<figure>
<img src="analysis/mpi_nodes_walltime_mkl.svg" alt="Figure 3: Parallel implementation wall time on a constant size problem, n=10000, as a function of mpi processes. For each mpi process there where two cores." id="fig:parallel_walltime2" /><figcaption>Figure 3: Parallel implementation wall time on a constant size problem, <span class="math inline">\(n=10000\)</span>, as a function of mpi processes. For each mpi process there where two cores.</figcaption>
</figure>
<h2 id="speedup"><span class="header-section-number">10.6</span> Speedup</h2>
<figure>
<img src="analysis/mpi_nodes_speedup.svg" alt="Figure 4: Speedup of program as a function of MPI processes when running on a constant problem size n=10000. For each MPI process there where two cores allocated to let OpenMP parallelize on the second core." id="fig:mpi_nodes_speedup" /><figcaption>Figure 4: Speedup of program as a function of MPI processes when running on a constant problem size <span class="math inline">\(n=10000\)</span>. For each MPI process there where two cores allocated to let OpenMP parallelize on the second core.</figcaption>
</figure>
<figure>
<img src="analysis/problem_size_speedup.svg" alt="Figure 5: Speedup of program as a function of problem size when running on a constant number of nodes, 5, MPI processes, 25, and cores, 50." id="fig:problem_size_speedup" /><figcaption>Figure 5: Speedup of program as a function of problem size when running on a constant number of nodes, 5, MPI processes, 25, and cores, 50.</figcaption>
</figure>
<h1 id="discussion"><span class="header-section-number">11</span> Discussion</h1>
<p>The speedup observed in fig. <a href="#fig:mpi_nodes_speedup">4</a> and fig. <a href="#fig:problem_size_speedup">5</a> will now be explained.</p>
<h2 id="speedup-as-a-function-of-problem-size"><span class="header-section-number">11.1</span> Speedup as a function of problem size</h2>
<p>Even though the data is noisy there seems to be a quite constant speedup of 15-20 times running on To explain this the following factors has to be taken into consideration.</p>
<p>5 nodes where used. Each node had 5 mpi processes and 10 cores available. The remaining 5 cores on each node would be saturated by the parallelism from the Intel MKL library. All in all this adds up to 50 cores.</p>
<p>Secondly and most importantly. The sequential implementation runs on one core. This gives it access to only that cores cache. Taking a closer look at the data sheet for the E5-2670<span class="citation" data-cites="intel-datasheet">[<a href="#ref-intel-datasheet">7</a>]</span> we observe that the cache hierarchy is the following:</p>
<ol type="1">
<li><p>L1 32kB instruction and 32kB data cache for each core</p></li>
<li><p>L2 256kB shared instruction data cache for each core</p></li>
<li><p>L3 20MB shared instruction data cache shared amongst all cores on the socket</p></li>
</ol>
<p>From a small calculation we see that for storing 3 1000x1000 matrices of doubles we need <span class="math display">\[3 \times 1000^2 \frac{64}{8} = 3 \times 7.68 \text{Mb} = 22.89 \text{Mb}\]</span> of memory. Thus even the smallest problem, <span class="math inline">\(n=1000\)</span> will fill all the cache layers from L1 to L3. For bigger problem this memory requirement increase on the order of <span class="math inline">\(O(n^2)\)</span>. When the cache is saturated the core will frequently have to go to RAM to get new parts of the problem. This memory access is several orders of magnitude slower that accessing the cache and the program sees a big slowdown because of it.</p>
<p>Adding all these factors together, 50 cores and a conservative 2 to 5 times slower memory access gives numbers close to <span class="math display">\[50 \times 2 = 100 &lt; \text{speedup} &lt; 50 \times 5 = 250.\]</span> This means that without taking into account the synchronization overhead the parallel algorithm could see between 100 and 250 times speedup in the given example. The reason we are seeing about 20 is because of all the synchronization introduced in the distributed algorithm.</p>
<h2 id="speedup-as-a-function-of-mpi-processes"><span class="header-section-number">11.2</span> Speedup as a function of MPI processes</h2>
<p>The same considerations apply to the speedup as a function of MPI processes. To explain the speedup of 30 you have to take into account that for each MPI process there where 2 cores allocated. This means we where using 72 cores. <span class="math display">\[72 \times 2 = 144 &lt; \text{speedup} &lt; 72 \times 5 = 360.\]</span> There is also here a considerable loss of speed because of synchronization. Nevertheless the speedup seem very linear following the formula <span class="math display">\[\text{speedup} = p\frac{3}{7}.\]</span> If you double the number of cores you get a bit more than 40% speed increase.</p>
<h2 id="feature-suggestions-and-research-questions"><span class="header-section-number">11.3</span> Feature suggestions and research questions</h2>
<p>The only big unanswered question is why a high level of SMP parallelism did not give better results than distributed memory parallelism. The highly hardware, library and compiler dependent nature of this problem makes it hard to debug on QUTs HPC cluster because of limited profiling possibilities etc. in a distributed MPI environment. Also the fact that you never are logged in to the cluster nodes itself where your program is running makes the whole debugging a lot harder. The authors theory is that the less L3 cache available for the versions running with higher SMP parallelizm ratio is considerably slowing down the application.</p>
<h1 id="conclusion"><span class="header-section-number">12</span> Conclusion</h1>
<p>The author considers the project a success both from a learning perspective and the actual results. The author learnt a whole array of new technologies that will be usefull in future work. The ability to use a real HPC facility for the experiments definitly helped motivate the work. Learning industry standard technologies as MPI, OpenMP and BLAS together with new mathematical analytical skills, good engineering practice for numerical codes etc. was very interesting. There is definitly possibilities for making a faster Poisson solver by eg. using FFT algorithms, but this was not seen as the goal of this report. The objective of making a general, fast and massively scalable Poisson solver was achieved. Acknowledgements to QUT HPC staff for being very helpfull with integration of the software into their platform.</p>
<h1 id="appendix-developed-code"><span class="header-section-number">13</span> Appendix: Developed code</h1>
<p>All the code developed for this project is open sourced and available in a version controled repository on the authors GitHub page <span class="citation" data-cites="github">[<a href="#ref-github">8</a>]</span>. The code run on the QUT HPC cluster is available under the summa folder. The LaTeXsource of this report is available under the folder <code>report</code>. All the timing data together with the R-script used for analysis of this is available under the folder <code>analysis</code>. There is also a working implementation of matrix multiply written in the accelerate domain specific language in Haskell under the <code>diagtest/diag-haskell</code> folder. The <code>doc</code> folder contains reference documentation and papers used during the creation of this report.</p>
<p>To run a desktop version of the application run on the HPC cluster follow the <code>README</code> in the <code>summa</code> folder.</p>
<h1 id="sec:math_der"><span class="header-section-number">14</span> Appendix: Derivation of solution method</h1>
<p>Poisson’s equation is given as <span class="math display">\[-\Delta u = f\]</span> where <span class="math inline">\(\Delta = \nabla^2\)</span> is the Laplace operator, u and f are real or complex valued functions in a euclidean space. The equation is often written as <span class="math display">\[-\nabla^2 u = f.\]</span> In two dimensions the equation can be written as <span class="math display">\[-\left( {\frac{\partial^2 }{\partial x^2}} + {\frac{\partial^2 }{\partial y^2}}\right) u(x,y) = f(x,y).\]</span></p>
<p>This equation is continuous and to work with it in the computer some discretized approximation has to be made. This equation is continuous and to work with it in the computer some discretized approximation has to be made. A finite difference approximation was chosen as it presents us with a problem that have several solution strategies that illustrates well the important properties to consider when designing parallel numerical algorithms.</p>
<p>After a FDM discretization we are left with a 2D regular grid. Each node in this grid represents a point for which we compute an approximation to our functions <span class="math inline">\(u(x,y)\)</span> and <span class="math inline">\(f(x,y)\)</span>. Using a central difference approximation to the partial derivatives in each direction leaves us with the following equation <span class="math display">\[-\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2} - \frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2} = f_{i,j}, 1\leq i,j \leq n-1.\]</span></p>
<p>The goal is to express this 2D equation as a linear system of equations which we then can solve with different techniques using the computer.</p>
<p>Let <span class="math display">\[U = \begin{bmatrix}
               u_{1,1} &amp; \dots &amp; u_{1,n} \\
               \vdots  &amp;       &amp; \vdots \\
               u_{n,1} &amp; \dots &amp; u_{n,n}
       \end{bmatrix}\]</span> be the discretized version of <span class="math inline">\(u\)</span>.</p>
<p>Let <span class="math display">\[T = \begin{bmatrix}
               2 &amp; -1 &amp; &amp; &amp; \\
               -1 &amp; 2 &amp; -1 &amp; &amp; \\
               &amp; \ddots &amp; \ddots &amp; \ddots &amp;\\
               &amp; &amp; -1 &amp; 2 &amp; -1\\
               &amp; &amp; &amp; -1 &amp; 2 \\
       \end{bmatrix}\]</span> be the discrete partial double derivative operator. Then, <span class="math display">\[\begin{aligned}
       (TU)_{i,j} &amp;= 2u_{i,j} - u_{i+1,j}, &amp;i=1,\\
       (TU)_{i,j} &amp;= 2u_{i,j} - u_{i+1,j} - u_{i-1,j}, &amp;2 \leq i \leq n-2,\\
       (TU)_{i,j} &amp;= 2u_{i,j} - u_{i-1,j}, &amp;i=n-1.\\\end{aligned}\]</span> and thus, <span class="math display">\[\frac{1}{h^2}(TU)_{i,j} \approx - \left( {\frac{\partial^2 u}{\partial x^2}} \right)_{i,j}.\]</span> By the same argument <span class="math display">\[\frac{1}{h^2}(UT)_{i,j} \approx - \left({\frac{\partial^2 u}{\partial y^2}}\right)_{i,j}.\]</span></p>
<p>Our finite difference scheme can thus be written as <span class="math display">\[\frac{1}{h^2}(TU + UT)_{i,j} = f_{i,j}, \quad 1\leq i,j \leq n-1.\]</span> Or <span class="math display">\[TU + UT = G\]</span> where <span id="eq:linsys"><span class="math display">\[
G = h^2 \begin{bmatrix}
               f_{1,1} &amp; \dots &amp; f_{1,n} \\
               \vdots  &amp;       &amp; \vdots \\
               f_{n,1} &amp; \dots &amp; f_{n,n}
       \end{bmatrix}.
\qquad(1)\]</span></span></p>
<p>The <span class="math inline">\(T\)</span> matrix may be diagonalized <span class="math display">\[T = Q \Lambda Q^\top\]</span> where <span class="math inline">\(\Lambda\)</span> is a diagonal matrix and <span class="math inline">\(Q Q^\top=I\)</span>, the identity matrix. When we insert this expression for <span class="math inline">\(T\)</span> in eq. <a href="#eq:linsys">1</a> we get <span class="math display">\[Q \Lambda Q^\top U + U Q \Lambda Q^\top = G.\]</span> Multiplying from right with <span class="math inline">\(Q\)</span> and left with <span class="math inline">\(Q^\top\)</span> gives <span class="math display">\[\begin{aligned}
&amp;(Q^\top Q) \Lambda Q^\top U Q + Q^\top U Q \Lambda (Q^\top Q)\\
&amp;= \Lambda Q^\top U Q + Q^\top U Q \Lambda = Q^\top G Q.\end{aligned}\]</span></p>
<h1 id="bibliography" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-summa">
<p>[1] R. A. Van De Geijn and J. Watts, “SUMMA: scalable universal matrix multiplication algorithm,” vol. 9. 1997.</p>
</div>
<div id="ref-mkl">
<p>[2] “mkl,” 20-Oct-2013. [Online]. Available: <a href="http://software.intel.com/en-us/intel-mkl" class="uri">http://software.intel.com/en-us/intel-mkl</a>.</p>
</div>
<div id="ref-blas">
<p>[3] “Blas,” 20-Oct-2013. [Online]. Available: <a href="http://www.netlib.org/blas/" class="uri">http://www.netlib.org/blas/</a>.</p>
</div>
<div id="ref-haskell">
<p>[4] “Haskell,” 20-Oct-2013. [Online]. Available: <a href="http://haskell.org/" class="uri">http://haskell.org/</a>.</p>
</div>
<div id="ref-accelerate">
<p>[5] “Accelerate,” 20-Oct-2013. [Online]. Available: <a href="http://hackage.haskell.org/package/accelerate" class="uri">http://hackage.haskell.org/package/accelerate</a>.</p>
</div>
<div id="ref-r-project">
<p>[6] “The R-Project,” 20-Oct-2013. [Online]. Available: <a href="http://www.r-project.org/" class="uri">http://www.r-project.org/</a>.</p>
</div>
<div id="ref-intel-datasheet">
<p>[7] “Intel® Xeon® Processor E5-1600/E5-2600/E5-4600 v2 Product Families,” Mar-2014. [Online]. Available: <a href="https://www-ssl.intel.com/content/dam/www/public/us/en/documents/datasheets/xeon-e5-v2-datasheet-vol-1.pdf" class="uri">https://www-ssl.intel.com/content/dam/www/public/us/en/documents/datasheets/xeon-e5-v2-datasheet-vol-1.pdf</a>.</p>
</div>
<div id="ref-github">
<p>[8] M. O. Lysgaard, “poisson,” 20-Oct-2013. [Online]. Available: <a href="https://github.com/molysgaard/poisson" class="uri">https://github.com/molysgaard/poisson</a>.</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="http://en.wikipedia.org/wiki/Box_plot" class="uri">http://en.wikipedia.org/wiki/Box_plot</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="http://software.intel.com/en-us/articles/intel-math-kernel-library-intel-mkl-using-intel-mkl-with-threaded-applications" class="uri">http://software.intel.com/en-us/articles/intel-math-kernel-library-intel-mkl-using-intel-mkl-with-threaded-applications</a><a href="#fnref2">↩</a></p></li>
</ol>
</section>

</div>
<div class="info">Posted by Morten Lysgaard on 2013-10-20. </div>
<div class="hide-on-print">
  <div class="info">Tags: .</div>
</div>

  </article>
  <footer class="hide-on-print">© 2011 -
    <script>document.write(new Date().getFullYear())</script> Morten Lysgaard. Licensed under GPL-v3 or later unless
    otherwise specified.
  </footer>

</body>

</html>

</html>